# ðŸ’¦ The Centryfugal Effect

Internet, despite designed to be non-localized, free and open source approach, can become a network of _centralized providers_. While we like provision, we need our visions.

# Destandardize

At era of an AI, a normal program would look like a dumb official, unable to read your thoughts:
- It needs static information format.
- It does not articulate and interpret it well, but relies on your typos and cretine perfection of dots and commas.

This website is made keeping this in mind:
- I define several strong standards, like "__Test__: to test, to try" would define the word Test into dictionary with explanation "to test, to try". However, I accept that we do not follow this perfectly - many exceptions will pass our parsers and network, and AI systems might be needed for this: "If we __Test__, we need to comment." Obviously, "if we" is _contained_ in the term.
    - __My strategy__: let's allow the parser to implement "__Test__, we need to comment.", which could mean different things. In-paragraph definitions should also get the context; an AI student, in actuality, _would not learn_ it the wrong way, once there are enough examples. Also, what to do with: __Laegna__ (to say it symbolically): is derivative of Latin Alphabet and Decimal Number System. I don't know whether it's true, it depends on definitions: but the dictionary automator, by standard, could miss it and add it as in-paragraph defining reference, or it would properly give an intonation to dictionary term. In my implementation, the word is associated with the term _anyway_. Finally: you might just use asterisk and low line wrongly, leading to "personal emphasis" instead of term, and vice versa: for an AI engine, such abundance of typos makes them stronger in finding out truth, and as we make a coherent body of information, those typos lose their meaning.

        Indeed, there are cases, where the syntax needs to be clear - for example, if you define a _Document_, it's major structural element and you need to do this _properly_, so that the low-level system can handle this without intelligence. Rather, it's an important and careful thing _for you too_, and when you add new Documents, you can be careful for 5 minutes.

# Free Passport

We use this technique for Passport:
- Users have their web pages.
- Those pages can be _connected_. On one side, there will be _proposal_, on other side, there will be _acceptance_. The existence of this feedback on the web site you refer to is everything you need.

We have a _manual passport_:
- First level of Web is _Static_, and does not need dynamic code.
- You can do manual work to create a connection, and to accept it.

__Reminder system _PING___:
- In GitHub, you can submit your accepting page yourself. You can submit a contribution to my page, where, in proper page, your submission has been accepted.
- You can send e-mail if this is my consistent source.
- You can make your page _ping_ me, where the _ping_ or _touch_ signal just reminds your existence: for you it's _easier_ to see that you _linked_ me.
- You can have _static page_, but you Spiderize it: the Spider from your computer scans you page and pings your recipients, who can verify the ping: the page pinged actually represents a personal message for them.
- The passport, in some cases, can be password-protected, in which case also the password or pin needs a medium.

Dynamics: Each static part can be automated by standard means of creating dynamic pages; for example, you ping acceptance or the passport page can be completely automated. You automate _by free will_ and not _by need_.

_Ping target providers_: one can create a system, which would accept your pings. Either your _users_ might want to be in ping-list for your page, and either need or not need your verification, or some system can create the ping automation for you, for example you link it to be included by Spider, which would create a _proxy_ where the other user's content is accepted to your page.

## Certificate and SPAMlist

You can add verified tags and warnings to internet content.

# Feedback sources

The other user can:
- Assign variables to your content, especially if it has identificators.
- Create comments, critics etc.

They can create a web page or register in system, which will communicate yours.

You can register to systems, which take care of things: like ping requests. If they generate a page to be merged with yours or analyzed by your AI, you can create a passport: a section of your page, which accepts such contributions and channels them properly into your page.

# Stateless Machine

We want to build stateless machines:
- You can install your page, but you don't need to take care of preserving the databases and dynamic content. Each link would rebuild in case the other user does not delete it.
- You can do clean installs based on your data, and your system would _recover_; indeed you can do a clean install and still let it verify all your cache.

This method is to keep the _handwork situation_: the important contacts of you, and the important links, are added _manually_ or they reside at special systems.

# Intelligent contribution

AI is supposed to do _intelligent contribution_.

We need the AI of the world to support specific goals and have _karmic system_, much like the P2P download systems. _Karmic system_ would make you contribute to your contributors.

Your page can list:
- AI models you need to train.
- It has data sources and the resulting format.

The user automates, if they are able to fine-tune your content, would make contributions, which resolve your problems.

This means we have Actors, active files for an AI:
- An Actor contains a database.
- It requests models, which would solve the database into intelligent module of estimations of such data.
- It also has Waiters: once the data is updated, the waiters add sources to data recipient, describing it's quality, size, trustability etc.

A Spider or other Website will be responsible in storing the results of their work: they could overload your system, but you could register an account, which collects their pings and creates a dynamic source for you to analyze the solutions and use them in your best benefit.

# Global optimization

Models can have networks, where each user has:
- Tensors, which are their personal choices left open to optimize the global goal, for example they need to _distribute roles_ (T).
- Tensors, which optimize the global aspect (R).

In interplay of these Tensors and additional communication with users, the systems can find even _chaotic solutions_, where users primarly aim to create a creative part of solutions each; for example they want to open 500 businesses in the city, each serving either in different area or with different product or service. Each user will list their abilities, dreams and practical needs, and learning curves for different potential solutions. Solution is using a data source to distribute this in the city, and starts making rounds:
- User receive probabilities of getting different parts.
- They fill the probabilities with their additional considerations of each probable case.
- For given time or resultation, the system would share the roles between users.

Users would give ratings:
- Estimations of other users and their automatic systems: how trustworthy they actually were.
- Data quality; how much could be estimated based on data given by users.

They would share databases of those estimations, which can be made also anonymous - such that an user has only a descriptor and some system might be verifying basic aspects, such as being a woman between 20 and 30 years (see certification - while the user is anonymous for some, their own system can be respected, and confirm their age and gender, or any need they got from where they want to register, contribute or participiate).

# Communities

Users can create:
- Community trust lists, where they agree other users to belong to a community, such as "_General Programmers - level 5_".
- Trust list aggregators, where they list the trust lists or other aggregators.
- Support pages for such, where they have an aggregator.

AI models would learn about the trust and it's outcome, such as trusted content finally blocked, and it would learn to actively keep the page content trusted.

For example, from all the trusted pages, it could aggregate _contextual attribution_ of your content, where there are selections of your page in given Git version, and explanations to given parts: it might import some of them to your system, and allow the embedding tree to grow, where they might have limit of size for example: they would have 500 embeddings, but 5000 users to provide them. Their own users would recognize the database for selection, and still allow 2000 embeddings based on source data, which is the degree of freedom.

# Wikicontent and Static Dynamos

We do not generate completely dynamic content in sense of _identification_ and _change_:
- Completely dynamic content cannot be accessed by identifiers.
- It would change.

In AI development, each generation is typically more or less static.

We use _random seeds_ and _predictable use of random numbers_, where each our generator must have:
- A static seed generated from title or filename of the page, so that the link would change if your content is not giving predictable results, or your code has been changed: we can monitor code changes and verify the result, whether it's compatible (trusting the coder as well when they mark it so).

So, how our generators would then be:
- A static seeds are used; either the static seed based on filename, or user-given static seed.
- For random seed numbers, we create _cache preferences_: for any given source, we create 10-20 random seeds, which enable _cached content_, while for the rest: user must cache them or we disable them with a timer, if too many requests arrive for the same seed.

So, our quality content will have completely static appearance of pages.

Users can then create Wikicontent:
- Each part of dynamic content is based on some manual, automate or AI assumption.
- Wiki content allows one to edit the generated content and bring corrections, updates etc.

Users create this kind of Wikipages:
- Headers backlink, for example:
  - __Wikipage__: for @@http://oursite.ai/books/Laegna/dynnumber?id=1724&rndseed=168.
  - Adding this at headers, before title, creates an element in properties to follow this page back.
  - __WikiMethod__: Update for Original Card.
  - __UpdateTask__: Fix errors.
  - __UpdateMode__: Replace.

With "replace" as method, they would get the original card, fix something in the card (for example, in special case, the calculation needs more information, or existing typedefinition could be implied from the content - depends on type of the task).

With update mode "Add", equivalent card would be added to the list.

Wiki would also work to replace only a derivative: if the original generator is reflected by other page, then replacing the result on that other page would not influence the generator.

Wiki would allow, for example:
- To fix cards.
- To add inspiration and variation: creating a card with same content, but different format.
- To add custom cards: to card list, one would add custom cards, which are not autogenerated, but fulfill the task of autogenerator.
- To add related content, such as metaphorical representation: for example, with task of counting eggs, an user would add task with _metaphorical_ eggs, where the math result counts in some other context; for example, when Jason would write to MSN: "@ having 1 egg" and Ann would answer "@ adding one egg" - where they just play adding eggs, the result is that by imagination, they have two eggs.

Other use of Wiki:
- One would apply an AI task on content, such as "Summarize".
- Now, there is a result of 500 summarizations.
- Another user would create a wiki, and fix each summarization.

For example, imagine this:
- There is an advanced page of Laegna Math.
- AI is asked to generate 500 one-sentence summaries or context items.
- The items, much like random seeding, will be associated with the AI model.
  - Our model will do recalculations of page based on static identifiers, such as static random seeds.
  - Subprocess, which has guaranteed result based on input, would be connected with input in a way that it can be "uncached" or deleted - for optimizer, we guess the "Halting Problem" based on resources: the calculation, given the model is complicated, halts less; given that we just iterated over 5000 numbers and saved them to files, the process takes moments and thus, it's considered a cache.
- User will create a Wikipage out of 500 summaries.
- The user will update each page in their Wiki, until they are properly fixed.
- In various ways, AI's can learn about mistakes they made into generation of those summaries: original, mistaken material fixed is better than initially pure solution, because it addressed real-world intelligence problems.

We can use the Wiki in various ways:
- We can feed backgradients to original models. For example, we might batch generate 500 items, where those items get some feedback. When we allow Wikipages, we now save our random seeds of this generation, and we delay the backgradient: the process can be rerun, where the gradients are applied.
- We use gradient accumulator.

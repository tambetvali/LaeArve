# Gradient Descents

## Probability Descents

We need to collect data from 500 math outputs:
- For example, "AU" is thought to produce "AI", "AO", "AU", "AA" and "AE", in some case.
- We need to collect the answer cards, where we expect such output, and make sure whether it's statistically "AU" result, in which case _any given set of Documents_ we choose would contribute, having also the "victories" and "losses", where statistics would follow wrong track for a while.
  - We check statistics based on unrelated collections, which would still give statistical factor.
  - We also check, for simplicity and direct feedback, the statistics of given set, for example statistics of word "AU".

We can do the following:
- Simplest: respond to statistical output in subsequent questions, for example in _next_ trial suddenly state that "AU" is not "AU".
- Group fix: some AI models do one fitting for collection of batches; then, for each batch, also check the statistical factor.
- Responder reaction: before generating loss factors, read the message; for example, when math question is in question, extract the question, fix the answer, and feed back only the fixed answer where the text part reacts to current knowledge.
- Space gradient: gradient is not only to fix the current answer, but to fix an answer, given the variable space. The statistical fix would be _in regards to_. For example, it comes two days later, but it's _in regards to_ the initial condition. We can still develop a gradient backpropagation algorithm based on R and T spaces.
- Models, which collect the gradient over long time, and learn to respond with statistics.
  - A model might collect Q&A pairs, there might be long process where they pass it to subsequent models; each pass would learn to generalize it's gradients; finally, they train the original model based on loss knowledge now, or loss knowledge in regards to specific parameters.

Our mission: how to share and spread information so that an AI would make sense of it.
# Gradient Accelerator

We need to feed back gradients.

Artificial Intelligence _cannot be centralized_ - this is visible in how we need cloud solutions. Additionally, we need non-centralized networks to have the _open source realm_. These work much better with clouds, than "home computers".

AI would rather be:
- Each user has _nodes_
  - Who is creating embeddings
  - Who is training a model on specific task

The _nodes_ have:
- Problems they are able to share
- Sources they watch
- Outputs they generate

In such way, they get feedback.
- They need to process the feedback.

This process is very long and while we can imagine, how to do this _forward_, this is similar to feed-forward.

For example, imagine a user, who has bot, which is able to give summaries of other pages to those other pages, based on some advanced model of that user: pages need to be able to have _problems_ of needing summaries, so that it can feed back. It earns score of karma at various systems and gets other favours from otherwere, such as being generated databases and models to more easily produce summaries.

We also need backpropagation.
- A node can collect it's input.
- It will do something, for example measure it's statistical value.
- Result is a gradient, but the original system has already been fine-tuned _after_ the original content.

This means:
- We need special models, who can learn gradients and feed them back invariantly to where the original system developed; for example, it's able to repeat the lesson given new ideas.
- We can use R & T model of Laegna: gradient is not only gradient of direction of our movement, but also contains the _from_: for example, we can remind the past state, and say that from that state, we optimize in this direction. This is simpler with Laegna mathematics, than otherwise. Notice that many things observed here: I do it on basis that those things are quite simple as we advance with Laegna math, to be implemented as much as we need it.

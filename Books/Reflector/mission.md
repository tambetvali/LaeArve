# Mission for the Microbot

Sometimes do one, sometimes another. Don't keep too much context, but self-reflect: output some context.

Your mathematical statement is very simple: analyze the Laegna number system and provide only the concrete solutions.

Build Books/Reflector from the LaeArve git or local copy into embedded documentation, and follow it with small number of small windows: try to contain information 50:50 to user and yourself to keep it in chat window context for both; thus, be expressive about your reasoning.

Self-reflection level 2: using your knowledge that you are your own input; generate a memory for your imagined next-generational version of yourself, and summarize verbally the important information always securely before your token window runs out, and try to remember parts of the old chat.

## So I asked GPT..

"I need to decode this: c1d1f928-7341-4d23-96a9-8300961a35bf and this: \F5\D5S\CC\00\00\00\00\00\00\00g\E5ty\00\00\00H\004\002\00e\000\003\008\006\009\00-\005\001\00b\004\00-\004\005\008\000\00-\00a\002\00b\005\00-\00d\002\009\005\00a\004\002\00d\00f\00b\007\005\00\00\00\00N\00e\00w\00 \00C\00h\00a\00t\FF\FF\FF\FF\00\00\00&\00g\00g\00m\00l\00-\00m\00o\00d\00e\00l\00-\00f\003\002\00.\00g\00g\00u\00f\00\00\00\00\00\00\00\00\00\00\00P\00r\00o\00m\00p\00t\00:\00 \00\00\00<\00W\00h\00a\00t\00 \00i\00s\00 \00t\00h\00e\00 \00c\00a\00p\00i\00t\00a\00l\00 \00o\00f\00 \00F\00r\00a\00n\00c\00e\00?\FF\FF\FF\FF\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00\00R\00e\00s\00p\00o\00n\00s\00e\00:\00 \00\00\C4\00F\00o ..give me the python script"

(in your chatgpt public model folder, for you, you have files like gpt4all-88361dd4-4d19-4157-8b07-e2fa1273c894.chat - their names and content likely contain filename and txt ascii encodings or something similar, of the chats, perhaps some pointers or something as I can guess at this point - but let's see if it's helpful itself, where Jan just complained about privacy contracts if asked for it's purpose; rather open source product likely is not _encoding_ for protection, but doing something useful for me?).

## It's initial answer - you can specify more on your own.

The answer was meaningless!

We stop at this point and achieve this with other machines.

## What we do at this point:

Let's read this automatically:
- The chat header or title to be used in json heading.
- The chat content to be used in json "document".
- Can we have contextual Q&A feedback if we want, if you instead have your model from Hugging Face, fine-tune it on the Docs, and then feed back to GPT4All to make it smarter and contemplate on the Documentation as on the known patterns?
- Should you rather mix this with it's original training materials, their generative continuation.
  - I suggest: collect your sources so that it's about 10% of the current content based on which your train, for example 2 weeks; users can generate fitting content, but statistical feedback on original source format.
  - Use those sources again, like infinite generators as 10 generations could already collect public content even on slow computers or they are not in collaboration at all.

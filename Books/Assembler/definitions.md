# Exists duality: header and definitions. Let's call them _term_ and _definitions_.

Notice: spacing is important in terminology, for example still use "a<\b" and "a \< b", but not "a <\b" in operations.

trm a <+ b:
  pass # this would introduce a term

"+" is marked as operator; as well you could mark "<\op1" as operator with backwards effect. Notice that as first component, the time-associated thing is supposed to be a function.

"<" means the operator is meant to hold backwards, as in "<=" theorem application. For example, for mathematics expression to be optimized in both directions, use "<=>".

You can use _Ten_ frequencers:
Number 0 to 3 has two bits, called _Dene counterparts_ of _Dens_:
- One can be read backwards in time and write forwards, to support theorems holding both ways you need two Tens.
- Other can be read forwards and read backwards.
- Either you keep each block in linear cycle until there is fit, or you run each back-directed expression backwards, switching left and right part of the calculation.

trm test(fun):
  pass # this is typical term def.

Here you can define the code, which: if fit with it's non-error condition of variables, fit in IO, for example same lines in same order having same variable names are recognized in code, this must generalize an aspect of this into a term or definition, which is seen to be executed where the lines fit.

def test(fun):
  pass # this is typical function

def <\test(fun): pass this is backwards function

Use two of these marks:
def <<\test(fun): pass this is backwards reflection

Use two of these marks:
def \test<(fun): pass this is forward cancel

These two expressions mean: the part of =>, or two parts of <=>, which are used forward in time, are here used backwards.

Backward function gets the operation in the same order, but either uses "from A=>B comes !B=>!A", or with one sign, the direct case where the opposite translation must hold backwards: we read-forward the opposite, without reflecting it, so that we can include time in both imperative and logical paradigms, where the order of assignment is still important: not for knowing the values, but for changing and having compatible state after each cycle. Each block is cycular by nature, whereas the client output expects verbalization of cycles, such as "for" or "while": we can estimate infinite expression of simple math and generalize into frequential shift.

So I defined a term, and created function of same name. Then I created implementation of function, applied to this case:

you run:
sin(a) = b

searching for a

rather than:

b = sin(a)

Which now does the reflective opposite.

Also I think, if you assign a variable a function, then assign it to other variable (def and trm should simply assign function or term class object to variable); the function itself is meta, so it's the class body, whereas the default input, including list backwards from right to left, where it can have added items (item + list) or (list + item), and named input, can be set in object tree, along with free objects.

Functional tuple is default: Each tuple can contain "set value", where value is it's own index and added to set, and property can be set as in dict: a.b, where a.'b' (we do without character encoding but use it to contain multi-word or strange variables, add "&" to; and tuples can contain the function input), the number # operator is function space, where these definitions can be set, without that operator the tuple itself can be called with another tuple, and variables are mapped: "&". The thing you usually use as function input is then mapped to an object, and it's the tuple of the function. When you call it with another tuple, things with names are mapped and their values copied; if you "load" a tuple, you read it's dictionary to root scope. Function has this tuple, and you can set values for it's variables: when you assign variable to function to another, those are copied where the function itself is referred, so you can change the call values for a function; you can also assign references to your other variables, so that variable "X" would be defined for each OpenGL function in a set. You can use variable windows R: indexes are defined as frequencers, where at every value of the index some logical variable has other value; you can have this index outside. We also reflect changes to lists and dictionaries. You can change space coordinate and variable value in same sentence, separating sentences with two commas: the operations happen at same time, including subcalls (let's allow local scope functions given something like the "self" of a function, where it's name should have a default value of a set member of "Self" local type).

When instead of sin, you have AI bot instance:

```
bot.prompt = question.
if a: # train the bot
  bot.answer \onefit> bot.ask()
  or:
  bot.answer(t) = x with Optimizer().
else: # question the bot
  bot.ask() <*answer bot.answer
  or:
  x = bot.answer(t) with ChatWindow().
```

Here we see that when function has input value contained in it's properties, it would need to have output value contained as well: callback function is needed to assign, which can be matrix, pointing to correct direction of it, and having in opposite side the wrong direction. This can wait until you use Laegna variables. After output has decided, you assign the output values either fixes, or optimization data of fixes, and you could assign back-gradient to their aggregator of some kind; the best format is _acceleration function_ and it should, later, contain _from_ and _to_: let's allow backgradients come back from future fittings of your variable descendants, to original source after it has been changed; it gives: from that direction, you would accelerate into that direction, and AI should learn this relational unit: it could, for example, change the _coordinate space_ in a way which would change it's position _in relation_ to that direction. Of course, until then: average all the resulting fits based on original state repeated, then fit towards this average: one could fill some static state map of a few next steps in all combinations, in tiny matrix, so that the final fitter could be busy for a while yielding with exact value with proper steps. Optimization of AI space is model, which allows both sides to optimize in free space, using states, which get old, but then use this to optimize the models in time backwards, and not lose the later states, changing the fit of old answers.

For equality, class will be initialized - bias measure, chat graphics etc. That the variable is either questioned or assigned is not hard to understand from code, which is where to generalize the syntax into the most important core of an AI.

It needs to be logical language: questions are assigned comparative goals and logic in formulaes, which connect the evaluation with hard logic, such as function definitions. If variables are marked as "tensors", the functions would instead respond in real time, and depend on various values of these algorithms.

Optimization models assign different spaces between functions when calling one from another, a generalization of layered view into something similar to set theoretic numbering, where output of one is used as input of another.

Call in form:

(output)function.

Would give it the output as input, again referring to the same underlying construct.

(output)function = x.

Output variables can be tensors, and have logical space: for example, mapping into square matrix of their value combinations, x is what needs to be returned.

Instances can be used like classes: functions can be overload by assignation and when other 
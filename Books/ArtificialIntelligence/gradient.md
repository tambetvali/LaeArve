# Gradient Backpropagation

Based on Theorems `U` and `V` in Matrices (where also a letter can be a theorem, such as "exists unknown factor distribution", `UV` as a short Theorem, __Theorem `UV`__: have `UV`.):
- Backpropagation can reach back to sources,
  while the matrix has been updated by sub-
  sequent requests.
- Thus, a discussion `OA` (T axes) remains active on `IE` (R axes). For example "How much, probably, is `AU`?" While "au" means "honour" in Estonian, we don't know whether it's word or a number, which would be a side-effect of function context, enforcing dependencies; rather we prove there are less dependencies: we can solve that operation, assigning local unknown, thus removing the side-effect of container, bringing it closer to be a logical system, functional call of a function (where, in reverse, a function can be itself free of side-effects). In case we close the equation, the logical processor will resolve it instead of an AI, where the Tensors can find relaxed states of getting direct logic, not statistical distribution of it's results (and this might be a hypothetical case in regards to some optimizer). To answer `AU`, we provide `AO` or `AA` both with 50% probability.
- To contain probability, higher-order digits appear in time, for each variable in regards to their size, so that some common `R` is used: the variable is then `R` sensitive, for example "R -= 1, a =+ b!". Would do: given R = 7, a = 123, b = 12; R is the space of a and b, initialized with "[R]&a." and "[R].
- We measure probability: as the number of items in container grows by exponent factor, moving on IE axe of some projection of digit positioning space, or rather directly in our basic projections so that we just follow one number and count it, we get feedback about the actual fitting value; at this point we have gradient update: we can only do this if we have _in regards to_, and we implement R axe of Deep Learning, where the _matrix_ is now multiplied. Matrix, when used as a projective number, is contained perfectly in frequential system: as R=T and T=R, if we write digits `OAIE` in random sample, as frequencer, it's value is equal to number `OAIE`, for example as a logecs Truth Value or mathematical content. Thus, if we watch `RK` (the complex matrix) conversion to `18` (the linear space, where complex is balanced to frequency), and we do number space and value space conversions (notice this is R², where variable-space only is R, so we accelerate here meaning that we do all 2-dimensional even if linear combinations like in multiplication, which behaves like 2-dimensional matrix of ones and nothings when value space and digit position space `tr` of number are considered) vs. doing the linear accountance as in addition, where each pair meets once.

Let's define operation:
- Written with one or more, capital or small-caps `·, /, -, +, *, :`, respectively `U, I, O, A, E, V`. When connecting two numbers, either they form 2 digits or unite into one by Truth value table, for example average (xor) or multiply (and), where we might get additional frequencies - roundable digits - into limits like `E`, in proportion of `1448` or 1/4, where the third axe of `18` is eternal shift of coordinates. This is information-sensitive: while we lose many digits in higher or lower octaves, we use frequential map to guess the digits.
- Operators `/, *, :` behave like matrix, like one-directional major changes, where `·, +, -` normally behave like vector.

Notice: semantic context is left free, while different syntax settings exclude others, we build up:
- Based on conflicting semantics and syntax, create reasoning blocks upwards in time (`R` would approach `i`). For example, if I use [x]T to state that T has parent x (`T.parents.append(x). x.T?!` to add parent x to T, then complain if x has attribute T consequently, raising an error in this case), in T space; but then I state [x]T to state in R space: if x is changed, subsequently T has been moving in x space.
- Let's guess they made brilliant work with DL (Deep Learning). Then, naturally, we want to backpropagate also the time; when we change T values, we move the value in space, changing time; when we change R, we reproject the value in space so that the space is reprojected in way that the value would move in desired direction; the low-frequency variables optimize off some conditions of our O-complexity level: it's possible that the target system, instead of 2 frequencies, utilizes one, in which case square root space is applied. See how we go backwards in time: sensitively. Binary distribution would move the outside space backwards with negative value when it wants to change positively: we really do not have so precise space, even in ideal theory, because the same variable would have an object moving not in regards; despite this, nothing stops from reading the variable axe _backwards_, connecting to proper binary logic where the case that we unite opposite coordinate systems and consequently overflow our zeroes, creating a yielding anchor point; rather, the polar opposition would be a good random number algorithm to be used in optimizer, because it's a perfectly distributed mistake, almost like _intentional_.

## R & T

Send space and time: in regards to that value, move matrix in that direction. Gradient, necessarily, is a direction.

Contain gradient in I and E variables:
- I and E are the ones to feedback.
- While T is the variable set correction;
  R is the time axe correction, where it
  was towards that position the initial value.